{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is to match face of a given photo with all the faces from photos folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d0a0a5436e4915a4fa0c0ccceab5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=111898327.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashwi\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mtcnn = MTCNN(image_size=240, margin=0, min_face_size=20) # initializing mtcnn for face detection\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() # initializing resnet for face img to embeding conversion\n",
    "\n",
    "dataset=datasets.ImageFolder('labeled_faces/test') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "face_list = [] # list of cropped faces from photos folder\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn(img, return_prob=True) \n",
    "    if face is not None and prob>0.90: # if face detected and porbability > 90%\n",
    "        emb = resnet(face.unsqueeze(0)) # passing cropped face into resnet model to get embedding matrix\n",
    "        embedding_list.append(emb.detach()) # resulten embedding matrix is stored in a list\n",
    "        name_list.append(idx_to_class[idx]) # names are stored in a list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data into data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2244e-02, -5.3660e-02, -6.0101e-02, -8.9254e-02,  1.1569e-02,\n",
       "         -5.1270e-03, -1.6831e-02, -4.4577e-03,  7.2051e-02, -6.0032e-02,\n",
       "         -7.4484e-02,  4.7916e-02,  5.4933e-02, -1.6775e-02, -6.7788e-03,\n",
       "          2.1315e-02, -2.4184e-02, -3.9030e-02,  5.4845e-02,  1.9401e-02,\n",
       "          3.6292e-02, -2.7346e-02,  4.0775e-02,  1.6314e-02,  8.5889e-02,\n",
       "          1.1859e-02, -3.6736e-02, -6.6740e-02,  8.9234e-03,  3.5632e-03,\n",
       "         -1.4481e-02, -2.7409e-02, -6.1107e-02,  2.7576e-02,  7.2388e-02,\n",
       "          1.2719e-02,  2.6347e-02, -3.0697e-02, -5.1767e-02,  5.9949e-02,\n",
       "          1.2951e-02,  3.0715e-02, -9.9562e-02,  2.1229e-02, -5.0728e-02,\n",
       "          1.4605e-02, -2.1627e-03,  1.3950e-02, -6.5230e-02, -4.6013e-03,\n",
       "         -3.7671e-02, -4.4239e-02, -7.3892e-02,  3.3165e-02, -1.0171e-02,\n",
       "          7.4949e-02,  2.1729e-02,  8.4439e-04,  9.1955e-03, -1.3587e-02,\n",
       "          5.1921e-02,  9.0390e-02, -5.3600e-02,  1.3235e-02,  1.9441e-02,\n",
       "          5.5693e-02,  2.9380e-02, -2.8018e-02,  7.2743e-03, -5.7146e-02,\n",
       "         -6.0554e-02, -1.4533e-02, -1.2846e-02,  4.7898e-02,  5.0693e-02,\n",
       "         -6.1454e-03, -5.2061e-02,  3.2911e-02,  3.0916e-03, -5.4747e-03,\n",
       "          3.1823e-02, -5.4950e-02,  1.0778e-02, -1.4777e-02, -2.6514e-02,\n",
       "         -8.8748e-03, -5.1471e-02,  6.7473e-02, -5.3656e-02,  5.5321e-02,\n",
       "          1.0965e-01, -8.5399e-03,  8.5687e-02, -2.5215e-02, -2.7462e-02,\n",
       "         -1.6784e-02,  1.5497e-02,  1.1148e-02, -5.2265e-02, -2.2608e-02,\n",
       "          1.7931e-02, -5.7838e-02,  3.5156e-02,  3.0967e-02, -7.0183e-03,\n",
       "         -2.0537e-02, -1.0113e-02, -3.3105e-02, -6.6130e-02,  7.7277e-03,\n",
       "         -1.6330e-03,  1.8095e-02,  3.0871e-03,  1.6477e-02,  2.8864e-02,\n",
       "         -6.0271e-02, -7.8182e-03,  2.9942e-02, -5.8496e-03,  3.8867e-02,\n",
       "         -2.4200e-02, -2.5351e-03,  4.7711e-02, -6.6688e-05,  2.6089e-02,\n",
       "          2.6884e-02, -1.0311e-02, -6.2803e-02, -5.9973e-02, -3.8014e-02,\n",
       "         -2.1932e-02,  3.8052e-02, -2.4838e-02, -8.3799e-02, -2.2806e-02,\n",
       "         -2.6889e-02,  4.0664e-02, -3.8482e-03,  3.1327e-03, -5.7826e-03,\n",
       "         -5.0297e-02, -2.7422e-02, -1.6240e-02,  4.4866e-02,  7.4571e-02,\n",
       "         -1.2835e-01, -2.9387e-02,  5.7530e-02,  8.1482e-03, -6.8122e-02,\n",
       "         -2.7442e-03, -5.0907e-02,  9.7482e-03,  3.5864e-02, -1.4335e-02,\n",
       "         -4.2317e-02,  3.6818e-02,  7.5036e-02,  2.5254e-03,  1.6591e-02,\n",
       "          5.0527e-02, -2.0780e-03, -8.0656e-02, -4.8775e-02,  7.3087e-02,\n",
       "          9.6975e-02, -2.1774e-03, -5.3583e-02,  3.4967e-02,  2.0204e-02,\n",
       "         -4.8087e-02, -1.6380e-03,  3.7242e-02, -5.2074e-02,  6.8117e-02,\n",
       "         -8.2518e-02,  1.4540e-02, -1.6494e-02, -7.7403e-02,  6.1894e-04,\n",
       "          2.4629e-02,  1.0827e-02, -5.5296e-02,  5.2618e-02, -9.1433e-03,\n",
       "          6.3832e-02,  7.9192e-03, -2.5272e-02, -1.5942e-02, -1.8119e-02,\n",
       "         -2.3240e-03, -4.2674e-02,  5.3559e-02, -4.8630e-03, -5.8484e-02,\n",
       "         -4.5570e-02,  6.9637e-02,  8.0785e-02,  1.4463e-02,  3.8539e-02,\n",
       "          2.7247e-02, -1.1551e-02, -2.9655e-02, -6.8890e-03,  8.7400e-04,\n",
       "          1.9053e-02, -2.3409e-02, -2.4209e-03,  9.6805e-02,  8.0245e-02,\n",
       "         -2.2137e-02, -7.6497e-03, -5.0639e-02, -2.7698e-02, -3.7795e-03,\n",
       "         -2.2327e-02, -6.8517e-02, -2.2449e-03, -1.1457e-02,  5.7632e-03,\n",
       "         -1.0856e-01, -5.7166e-02, -3.6846e-02, -6.8584e-02, -1.3165e-02,\n",
       "          3.9094e-02, -1.2517e-02, -4.2851e-02, -1.8742e-02, -1.4386e-02,\n",
       "         -3.9314e-03,  2.5723e-03, -4.1788e-03, -5.4237e-02,  9.8263e-02,\n",
       "          6.1544e-02,  7.0990e-03, -4.7107e-02, -3.7541e-02,  2.9073e-02,\n",
       "          2.9414e-02,  5.4681e-02,  2.2415e-02,  3.9542e-02, -7.6261e-02,\n",
       "          4.1757e-02,  1.2554e-02,  6.0476e-02, -7.8359e-02, -3.4891e-04,\n",
       "          7.6380e-02,  4.5525e-02,  1.4163e-02, -2.1158e-03,  4.1407e-02,\n",
       "         -1.3788e-02, -1.9904e-02,  2.8176e-02,  3.1764e-02,  7.1214e-02,\n",
       "         -1.6192e-02, -3.7573e-02, -5.1038e-02,  9.8902e-03, -2.6053e-02,\n",
       "          6.6535e-02,  3.4090e-02,  5.8848e-02, -8.5902e-02, -3.5955e-03,\n",
       "         -3.3704e-02, -3.2288e-02, -3.3894e-02, -4.3173e-03,  3.6780e-02,\n",
       "         -7.3793e-03,  3.8362e-02, -1.6102e-02, -1.9955e-02, -6.0929e-02,\n",
       "         -5.5982e-03, -1.1069e-02, -1.8920e-03,  1.5949e-02, -4.2105e-03,\n",
       "         -7.3257e-02, -5.4749e-02, -5.1456e-02,  2.4287e-02,  5.4658e-02,\n",
       "          7.4533e-03, -2.5648e-02, -1.9797e-02,  6.2905e-03, -1.0149e-02,\n",
       "          2.3664e-02,  1.6240e-02,  7.8040e-03, -4.5772e-02,  4.0527e-02,\n",
       "          3.9998e-03,  1.7262e-02,  4.9804e-02,  9.3101e-02, -2.3153e-02,\n",
       "         -2.5328e-02,  5.7741e-03, -3.4685e-02,  3.1525e-02, -5.3840e-02,\n",
       "         -5.5295e-02,  2.4392e-04,  6.8781e-02,  1.9439e-02, -1.7416e-02,\n",
       "         -2.1906e-02, -8.6845e-02,  3.7274e-02, -1.6262e-02,  2.0651e-02,\n",
       "         -3.5642e-02, -5.3619e-02, -3.7959e-02,  3.6542e-02, -1.0953e-01,\n",
       "          3.3776e-02,  6.5022e-02, -3.5008e-02, -7.3906e-02,  7.4460e-02,\n",
       "         -2.1591e-02, -6.9892e-02, -2.9677e-02, -6.2060e-03, -4.9737e-03,\n",
       "         -6.3247e-02,  2.2704e-02, -4.5042e-03,  9.6619e-02,  6.8275e-02,\n",
       "          4.3047e-02, -2.5126e-02, -7.9681e-03, -2.6028e-02, -4.0419e-02,\n",
       "         -2.5418e-02,  4.4130e-02,  3.2615e-02, -7.3109e-02,  7.2325e-02,\n",
       "          1.9847e-02,  4.8582e-02,  8.2454e-02, -8.1507e-03, -8.1318e-02,\n",
       "          7.6644e-03,  8.9318e-02, -7.0833e-02, -3.9426e-04, -3.2848e-02,\n",
       "         -1.7721e-02, -3.1370e-02,  1.2598e-02, -5.8711e-03,  1.9795e-02,\n",
       "          8.1514e-02,  7.1058e-02, -1.9037e-02, -1.3464e-02,  3.7475e-02,\n",
       "         -7.6637e-02,  4.4361e-03, -3.8617e-03,  9.8295e-03,  2.9435e-02,\n",
       "         -7.1461e-03,  4.9241e-02, -9.2339e-02,  1.8977e-02,  8.4568e-02,\n",
       "          2.2767e-02, -3.2806e-02,  6.6560e-02,  3.2303e-02,  1.3787e-02,\n",
       "          1.4875e-03,  2.3256e-03, -1.1785e-02,  1.6301e-02,  1.8269e-02,\n",
       "         -1.2368e-02,  3.4576e-02, -1.3706e-02,  2.5900e-02,  2.7425e-02,\n",
       "          5.0311e-02,  9.8906e-02,  4.2444e-02, -3.4464e-02, -3.8614e-02,\n",
       "         -2.2575e-02,  1.5196e-02,  2.2560e-02, -1.1247e-02, -5.9096e-02,\n",
       "          1.2415e-03, -2.9657e-02, -1.2425e-02,  9.9843e-03, -6.8530e-03,\n",
       "          3.1107e-04, -1.6279e-02,  3.6143e-03, -2.3467e-02, -6.3620e-02,\n",
       "         -3.5454e-02,  7.1030e-02, -4.7179e-02,  1.9924e-02, -6.7098e-02,\n",
       "         -9.2678e-02,  1.1649e-01, -2.9563e-02,  6.6241e-02,  3.8589e-02,\n",
       "          1.7066e-02, -7.9300e-04, -6.0160e-02, -3.7996e-02, -3.9598e-02,\n",
       "          6.5912e-02,  4.5959e-02,  5.0730e-03,  4.8330e-02, -4.4141e-02,\n",
       "          5.1108e-02,  1.4500e-01,  9.5696e-02, -2.5057e-02,  1.4836e-02,\n",
       "         -1.0676e-02, -1.5056e-02,  2.1243e-03,  8.1836e-03,  1.0217e-01,\n",
       "          2.1524e-02, -1.2122e-02, -7.1448e-02, -5.0983e-03,  3.4996e-02,\n",
       "          1.7136e-02,  1.4622e-02,  2.5639e-02, -1.3838e-02, -9.4627e-03,\n",
       "         -4.0571e-02,  6.5897e-02,  5.4234e-02,  5.0858e-02, -2.6406e-02,\n",
       "          1.0167e-02, -1.7781e-02, -3.3796e-02, -5.8745e-02,  2.1842e-02,\n",
       "         -1.4403e-02, -1.3217e-01, -1.1325e-02,  6.9884e-02,  1.4837e-02,\n",
       "          6.4824e-03,  7.9263e-02,  9.0283e-05, -1.2156e-03,  1.7655e-02,\n",
       "         -5.2499e-02, -8.9410e-03, -4.3336e-02, -8.0515e-03,  2.2860e-02,\n",
       "          3.5168e-02,  1.8327e-02,  6.1163e-02,  1.0221e-01,  9.0322e-02,\n",
       "          2.8335e-02, -4.2905e-05, -5.3338e-02,  4.5724e-02, -2.4197e-02,\n",
       "         -2.6153e-02, -7.6510e-02,  3.5272e-02,  4.9258e-02,  8.4632e-02,\n",
       "          5.1163e-02, -3.7544e-02, -2.0504e-02, -5.9193e-03, -1.9482e-02,\n",
       "         -4.0515e-02, -3.9914e-02, -3.6671e-02, -1.2121e-02, -3.6305e-02,\n",
       "          3.6318e-03,  2.6519e-02,  3.0162e-02, -4.5385e-03, -1.0936e-02,\n",
       "          2.4797e-02, -5.3001e-02]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [embedding_list, name_list]\n",
    "torch.save(embedding_list, 'data.npy') # saving data.pt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching face id of the given photo with available data from data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-93b201fa4545>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labeled_faces/train/priyanka_chopra/2.1.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# Detect faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mbatch_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;31m# Select faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_all\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             batch_boxes, batch_points = detect_face(\n\u001b[0m\u001b[0;32m    314\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_face_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001b[0m in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MTCNN batch processing only compatible with equal-dimension images.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MTCNN batch processing only compatible with equal-dimension images.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "img,lbl = mtcnn('labeled_faces/train/priyanka_chopra/2.1.jpg', return_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-761b4d17bab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labeled_faces/train/priyanka_chopra/2.1.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Face matched with: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'With distance: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-761b4d17bab7>\u001b[0m in \u001b[0;36mface_match\u001b[1;34m(img_path, data_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# getting embedding matrix of the given img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# returns cropped face and probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# detech is to make required gradient false\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# Detect faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mbatch_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;31m# Select faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_all\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             batch_boxes, batch_points = detect_face(\n\u001b[0m\u001b[0;32m    314\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_face_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001b[0m in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mmodel_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "def face_match(img_path, data_path): # img_path= location of photo, data_path= location of data.pt \n",
    "    # getting embedding matrix of the given img\n",
    "    img = Image.open(img_path)\n",
    "    face, prob = mtcnn(img, return_prob=True) # returns cropped face and probability\n",
    "    emb = resnet(face.unsqueeze(0)).detach() # detech is to make required gradient false\n",
    "    \n",
    "    saved_data = torch.load('data.pt') # loading data.pt file\n",
    "    embedding_list = saved_data[0] # getting embedding data\n",
    "    name_list = saved_data[1] # getting list of names\n",
    "    dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "    \n",
    "    for idx, emb_db in enumerate(embedding_list):\n",
    "        dist = torch.dist(emb, emb_db).item()\n",
    "        dist_list.append(dist)\n",
    "        \n",
    "    idx_min = dist_list.index(min(dist_list))\n",
    "    return (name_list[idx_min], min(dist_list))\n",
    "\n",
    "\n",
    "result = face_match('labeled_faces/train/priyanka_chopra/2.1.jpg', 'data.pt')\n",
    "\n",
    "print('Face matched with: ',result[0], 'With distance: ',result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROSA's IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models,transforms,utils,datasets\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "import time,os,copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'labeled_faces/train/'\n",
    "\n",
    "VAL_PATH = 'labeled_faces/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-116-9b63e3a2d681>:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  train_data_np = np.array(train_data)\n",
      "<ipython-input-116-9b63e3a2d681>:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val_data_np = np.array(val_data)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from ImageFolder\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'labeled_faces/'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "                  \n",
    "# Save as npy files\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for img, lbl in image_datasets['train']:\n",
    "    train_data.append([np.array(img), np.array(lbl)])\n",
    "    \n",
    "train_data_np = np.array(train_data)\n",
    "np.save('train_data' + '.npy', train_data_np)\n",
    "\n",
    "val_data = []\n",
    "\n",
    "for img, lbl in image_datasets['test']:\n",
    "    val_data.append([np.array(img), np.array(lbl)])\n",
    "    \n",
    "val_data_np = np.array(val_data)\n",
    "np.save('val_data' + '.npy', val_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, datafile):\n",
    "        self.datafile = datafile\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datafile.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.datafile[idx][0]\n",
    "        y = int(self.datafile[idx][1])\n",
    "        if y == 0:\n",
    "            lbl = torch.tensor([1.0,0.0,0.0])\n",
    "        elif y == 1:\n",
    "            lbl = torch.tensor([0.0,1.0,0.0])\n",
    "        else:\n",
    "            lbl = torch.tensor([0.0,0.0,1.0])\n",
    "\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('train_data.npy',allow_pickle=True)\n",
    "val_data = np.load('val_data.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_class = Data(train_data)\n",
    "val_data_class = Data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data_class,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_data_class,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-1.9637812 , -1.9980307 , -1.9980307 , ..., -1.5699118 ,\n",
       "          -1.6041614 , -1.6726604 ],\n",
       "         [-1.980906  , -1.980906  , -1.980906  , ..., -1.7411594 ,\n",
       "          -1.7754089 , -1.5185376 ],\n",
       "         [-1.9980307 , -1.980906  , -1.9637812 , ..., -1.6555357 ,\n",
       "          -1.6555357 , -1.6555357 ],\n",
       "         ...,\n",
       "         [-1.3301654 , -1.2959158 , -1.2102921 , ..., -1.8096584 ,\n",
       "          -1.4671633 , -1.2959158 ],\n",
       "         [-1.2445416 , -1.2274169 , -1.1931673 , ..., -1.6897851 ,\n",
       "          -1.7754089 , -1.5356624 ],\n",
       "         [-1.1931673 , -1.2102921 , -1.1931673 , ..., -1.7582842 ,\n",
       "          -1.7582842 , -1.7582842 ]],\n",
       " \n",
       "        [[-1.8781512 , -1.9131652 , -1.9131652 , ..., -1.4754901 ,\n",
       "          -1.5105042 , -1.5805322 ],\n",
       "         [-1.8956583 , -1.8956583 , -1.8956583 , ..., -1.6505601 ,\n",
       "          -1.6855742 , -1.4229691 ],\n",
       "         [-1.9131652 , -1.8956583 , -1.8781512 , ..., -1.5630252 ,\n",
       "          -1.5630252 , -1.5630252 ],\n",
       "         ...,\n",
       "         [-1.230392  , -1.1953781 , -1.107843  , ..., -1.7205882 ,\n",
       "          -1.370448  , -1.1953781 ],\n",
       "         [-1.1428571 , -1.12535   , -1.0903361 , ..., -1.5980392 ,\n",
       "          -1.6855742 , -1.4404761 ],\n",
       "         [-1.0903361 , -1.107843  , -1.0903361 , ..., -1.6680672 ,\n",
       "          -1.6680672 , -1.6680672 ]],\n",
       " \n",
       "        [[-1.6475817 , -1.68244   , -1.68244   , ..., -1.2467101 ,\n",
       "          -1.2815686 , -1.3512855 ],\n",
       "         [-1.6650109 , -1.6650109 , -1.6650109 , ..., -1.4210021 ,\n",
       "          -1.4558606 , -1.1944225 ],\n",
       "         [-1.68244   , -1.6650109 , -1.6475817 , ..., -1.3338562 ,\n",
       "          -1.3338562 , -1.3338562 ],\n",
       "         ...,\n",
       "         [-1.0027015 , -0.9678431 , -0.88069713, ..., -1.490719  ,\n",
       "          -1.1421349 , -0.9678431 ],\n",
       "         [-0.91555554, -0.8981263 , -0.86326796, ..., -1.3687146 ,\n",
       "          -1.4558606 , -1.2118517 ],\n",
       "         [-0.86326796, -0.88069713, -0.86326796, ..., -1.4384314 ,\n",
       "          -1.4384314 , -1.4384314 ]]], dtype=float32),\n",
       " tensor([1., 0., 0.]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "dataset_sizes = {'train': train_data.shape[0], 'val': val_data.shape[0]}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model as feature extractor\n",
    "model_conv = models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Sequential(nn.Linear(num_ftrs, 3), torch.nn.Sigmoid())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as opposed to before.\n",
    "#optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_conv = optim.Adam(model_conv.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1173, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing with one batch\n",
    "\n",
    "img, lbl = next(iter(dataloaders['val']))\n",
    "\n",
    "outputs = model_conv(img)\n",
    "value, preds = torch.max(outputs, 1)\n",
    "loss = criterion(outputs, lbl)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer,criterion, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = 1.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    value, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                running_corrects += torch.sum(preds == torch.argmax(labels,1)) #labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.3818\n",
      "val Loss: 0.2721 Acc: 0.2500\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 0.5227\n",
      "val Loss: 0.2139 Acc: 0.3750\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.1670 Acc: 0.6727\n",
      "val Loss: 0.2208 Acc: 0.5208\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.7545\n",
      "val Loss: 0.1978 Acc: 0.6042\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.8000\n",
      "val Loss: 0.1717 Acc: 0.7083\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1191 Acc: 0.8455\n",
      "val Loss: 0.1669 Acc: 0.7083\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.8682\n",
      "val Loss: 0.1378 Acc: 0.7917\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1005 Acc: 0.9182\n",
      "val Loss: 0.1252 Acc: 0.8542\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.9000\n",
      "val Loss: 0.1143 Acc: 0.8750\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9136\n",
      "val Loss: 0.1094 Acc: 0.8958\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0940 Acc: 0.9364\n",
      "val Loss: 0.1070 Acc: 0.9167\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0936 Acc: 0.9182\n",
      "val Loss: 0.1058 Acc: 0.9167\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0958 Acc: 0.9227\n",
      "val Loss: 0.1060 Acc: 0.9167\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9091\n",
      "val Loss: 0.1049 Acc: 0.9167\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0936 Acc: 0.9273\n",
      "val Loss: 0.1045 Acc: 0.9167\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9136\n",
      "val Loss: 0.1041 Acc: 0.9167\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0976 Acc: 0.9000\n",
      "val Loss: 0.1043 Acc: 0.9167\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0939 Acc: 0.9318\n",
      "val Loss: 0.1044 Acc: 0.9167\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0940 Acc: 0.9273\n",
      "val Loss: 0.1041 Acc: 0.9167\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0907 Acc: 0.9455\n",
      "val Loss: 0.1042 Acc: 0.9167\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0953 Acc: 0.9000\n",
      "val Loss: 0.1043 Acc: 0.9167\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9091\n",
      "val Loss: 0.1046 Acc: 0.9167\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0948 Acc: 0.9364\n",
      "val Loss: 0.1042 Acc: 0.9167\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0930 Acc: 0.9227\n",
      "val Loss: 0.1040 Acc: 0.9167\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0935 Acc: 0.9091\n",
      "val Loss: 0.1045 Acc: 0.9167\n",
      "\n",
      "Training complete in 3m 14s\n",
      "Best val Acc: 0.916667\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv,  optimizer_conv, criterion,  exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(),'models/model-resnet18-2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_class[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open('labeled_faces/train/priyanka_chopra/2.1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-7d911b0bc09c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# See note [TorchScript super()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mNoneType\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "model_conv(val_data_class[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
